2019-07-12 04:05:44 INFO     Loading Data
2019-07-12 04:05:49 INFO     Model Setting Up
2019-07-12 04:05:59 INFO     train_dir : [../dataset/mnist/train]
2019-07-12 04:05:59 INFO     test_dir : [../dataset/mnist/test]
2019-07-12 04:05:59 INFO     save_dir : [./save]
2019-07-12 04:05:59 INFO     gpu : [5]
2019-07-12 04:05:59 INFO     num_epochs : [300]
2019-07-12 04:05:59 INFO     batch_size : [256]
2019-07-12 04:05:59 INFO     num_points : [512]
2019-07-12 04:05:59 INFO     visualize : [False]
2019-07-12 04:05:59 INFO     log_interval : [1000]
2019-07-12 04:05:59 INFO     save_interval : [20]
2019-07-12 04:05:59 INFO     baselr : [5e-05]
2019-07-12 04:05:59 INFO     density_radius : [0.2]
2019-07-12 04:05:59 INFO     num_grids : [3]
2019-07-12 04:05:59 INFO     base_radius : [1]
2019-07-12 04:05:59 INFO     static_sigma : [0.05]
2019-07-12 04:05:59 INFO     use_static_sigma : [False]
2019-07-12 04:05:59 INFO     use_weights : [False]
2019-07-12 04:05:59 INFO     sigma_layer_diff : [True]
2019-07-12 04:05:59 INFO     feature_out1 : [8]
2019-07-12 04:05:59 INFO     feature_out2 : [16]
2019-07-12 04:05:59 INFO     feature_out3 : [32]
2019-07-12 04:05:59 INFO     feature_out4 : [64]
2019-07-12 04:05:59 INFO     feature_out5 : [128]
2019-07-12 04:05:59 INFO     num_classes : [10]
2019-07-12 04:05:59 INFO     num_so3_layers : [3]
2019-07-12 04:05:59 INFO     bandwidth_0 : [10]
2019-07-12 04:05:59 INFO     bandwidth_out1 : [10]
2019-07-12 04:05:59 INFO     bandwidth_out2 : [8]
2019-07-12 04:05:59 INFO     bandwidth_out3 : [6]
2019-07-12 04:05:59 INFO     bandwidth_out4 : [4]
2019-07-12 04:05:59 INFO     bandwidth_out5 : [2]
2019-07-12 04:05:59 INFO     resume_training : [0]
2019-07-12 04:05:59 INFO     Start Training
2019-07-12 04:06:10 INFO     Batch: [0/235] Epoch: [0] Loss: [2.325087070465088]
2019-07-12 04:06:11 INFO     Batch: [1/235] Epoch: [0] Loss: [2.3206437826156616]
2019-07-12 04:06:11 INFO     Batch: [2/235] Epoch: [0] Loss: [2.324291944503784]
2019-07-12 04:06:11 INFO     Batch: [3/235] Epoch: [0] Loss: [2.3198485374450684]
2019-07-12 04:06:12 INFO     Batch: [4/235] Epoch: [0] Loss: [2.31925311088562]
2019-07-12 04:06:12 INFO     Batch: [5/235] Epoch: [0] Loss: [2.317214846611023]
2019-07-12 04:06:12 INFO     Batch: [6/235] Epoch: [0] Loss: [2.3144863673618863]
2019-07-12 04:06:13 INFO     Batch: [7/235] Epoch: [0] Loss: [2.3136521577835083]
2019-07-12 04:06:13 INFO     Batch: [8/235] Epoch: [0] Loss: [2.3127739164564343]
2019-07-12 04:06:13 INFO     Batch: [9/235] Epoch: [0] Loss: [2.3113605260848997]
2019-07-12 04:06:14 INFO     Batch: [10/235] Epoch: [0] Loss: [2.309961535713889]
2019-07-12 04:06:14 INFO     Batch: [11/235] Epoch: [0] Loss: [2.3088063995043435]
2019-07-12 04:06:14 INFO     Batch: [12/235] Epoch: [0] Loss: [2.3088078132042518]
2019-07-12 04:06:15 INFO     Batch: [13/235] Epoch: [0] Loss: [2.3086983306067332]
2019-07-12 04:06:15 INFO     Batch: [14/235] Epoch: [0] Loss: [2.308614238103231]
2019-07-12 04:06:15 INFO     Batch: [15/235] Epoch: [0] Loss: [2.3072056621313095]
2019-07-12 04:06:16 INFO     Batch: [16/235] Epoch: [0] Loss: [2.306107394835528]
2019-07-12 04:06:16 INFO     Batch: [17/235] Epoch: [0] Loss: [2.3043055401908026]
2019-07-12 04:06:16 INFO     Batch: [18/235] Epoch: [0] Loss: [2.303759110601325]
2019-07-12 04:06:17 INFO     Batch: [19/235] Epoch: [0] Loss: [2.303081011772156]
2019-07-12 04:06:17 INFO     Batch: [20/235] Epoch: [0] Loss: [2.301764885584513]
2019-07-12 04:06:17 INFO     Batch: [21/235] Epoch: [0] Loss: [2.300822604786266]
2019-07-12 04:06:18 INFO     Batch: [22/235] Epoch: [0] Loss: [2.3001212140788203]
2019-07-12 04:06:18 INFO     Batch: [23/235] Epoch: [0] Loss: [2.299184173345566]
2019-07-12 04:06:18 INFO     Batch: [24/235] Epoch: [0] Loss: [2.2982604789733885]
2019-07-12 04:06:19 INFO     Batch: [25/235] Epoch: [0] Loss: [2.2973272525347195]
2019-07-12 04:06:19 INFO     Batch: [26/235] Epoch: [0] Loss: [2.2964159735926875]
2019-07-12 04:06:19 INFO     Batch: [27/235] Epoch: [0] Loss: [2.2955873778888156]
2019-07-12 04:06:20 INFO     Batch: [28/235] Epoch: [0] Loss: [2.294928262973654]
2019-07-12 04:06:20 INFO     Batch: [29/235] Epoch: [0] Loss: [2.294049294789632]
2019-07-12 04:06:20 INFO     Batch: [30/235] Epoch: [0] Loss: [2.2930100733234036]
2019-07-12 04:06:21 INFO     Batch: [31/235] Epoch: [0] Loss: [2.2922158986330032]
2019-07-12 04:06:21 INFO     Batch: [32/235] Epoch: [0] Loss: [2.2911681045185435]
2019-07-12 04:06:21 INFO     Batch: [33/235] Epoch: [0] Loss: [2.2907934819950775]
2019-07-12 04:06:22 INFO     Batch: [34/235] Epoch: [0] Loss: [2.289768089566912]
2019-07-12 04:06:22 INFO     Batch: [35/235] Epoch: [0] Loss: [2.288993457953135]
2019-07-12 04:06:22 INFO     Batch: [36/235] Epoch: [0] Loss: [2.2880370294725574]
2019-07-12 04:06:23 INFO     Batch: [37/235] Epoch: [0] Loss: [2.287217855453491]
2019-07-12 04:06:23 INFO     Batch: [38/235] Epoch: [0] Loss: [2.286245945172432]
2019-07-12 04:06:23 INFO     Batch: [39/235] Epoch: [0] Loss: [2.285523867607117]
2019-07-12 04:06:24 INFO     Batch: [40/235] Epoch: [0] Loss: [2.284637113896812]
2019-07-12 04:06:24 INFO     Batch: [41/235] Epoch: [0] Loss: [2.283743216877892]
2019-07-12 04:06:24 INFO     Batch: [42/235] Epoch: [0] Loss: [2.282791814138723]
2019-07-12 04:06:25 INFO     Batch: [43/235] Epoch: [0] Loss: [2.2819015708836643]
2019-07-12 04:06:25 INFO     Batch: [44/235] Epoch: [0] Loss: [2.2808299276563857]
2019-07-12 04:06:25 INFO     Batch: [45/235] Epoch: [0] Loss: [2.2799768810686856]
2019-07-12 04:06:26 INFO     Batch: [46/235] Epoch: [0] Loss: [2.279565243010825]
2019-07-12 04:06:26 INFO     Batch: [47/235] Epoch: [0] Loss: [2.2790224899848304]
2019-07-12 04:06:26 INFO     Batch: [48/235] Epoch: [0] Loss: [2.278002923848678]
2019-07-12 04:06:27 INFO     Batch: [49/235] Epoch: [0] Loss: [2.2772820806503296]
2019-07-12 04:06:27 INFO     Batch: [50/235] Epoch: [0] Loss: [2.2766606059728884]
2019-07-12 04:06:27 INFO     Batch: [51/235] Epoch: [0] Loss: [2.2755976365162778]
2019-07-12 04:06:28 INFO     Batch: [52/235] Epoch: [0] Loss: [2.2746095252486893]
2019-07-12 04:06:28 INFO     Batch: [53/235] Epoch: [0] Loss: [2.273815910021464]
